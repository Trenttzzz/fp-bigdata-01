{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fec11ed3-efc7-4a3f-9748-b339c31f2d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Spark Session initialized successfully with Kafka, Delta Lake, and S3 connectors!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "# ADDED 'udf' to the import list\n",
    "from pyspark.sql.functions import from_json, col, when, to_timestamp, length, udf\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, IntegerType, DoubleType\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Define ALL required packages: Kafka, Delta Lake, and S3/Hadoop\n",
    "required_packages = [\n",
    "    \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0\",\n",
    "    \"io.delta:delta-spark_2.12:3.2.0\",\n",
    "    \"org.apache.hadoop:hadoop-aws:3.3.4\",\n",
    "    \"com.amazonaws:aws-java-sdk-bundle:1.12.262\"\n",
    "]\n",
    "\n",
    "# Join the packages into a comma-separated string for Spark configuration\n",
    "spark_packages = \",\".join(required_packages)\n",
    "\n",
    "# Initialize Spark Session with all required packages\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AmazonReviewsStreamingETL\") \\\n",
    "    .config(\"spark.jars.packages\", spark_packages) \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Reduce log verbosity\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(\"✅ Spark Session initialized successfully with Kafka, Delta Lake, and S3 connectors!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b092ef2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Kafka stream connected and data parsing is configured.\n"
     ]
    }
   ],
   "source": [
    "# Define the schema for the incoming Amazon review data\n",
    "review_schema = StructType([\n",
    "    StructField(\"Id\", LongType(), True),\n",
    "    StructField(\"ProductId\", StringType(), True),\n",
    "    StructField(\"UserId\", StringType(), True),\n",
    "    StructField(\"ProfileName\", StringType(), True),\n",
    "    StructField(\"HelpfulnessNumerator\", IntegerType(), True),\n",
    "    StructField(\"HelpfulnessDenominator\", IntegerType(), True),\n",
    "    StructField(\"Score\", IntegerType(), True),\n",
    "    StructField(\"Time\", LongType(), True),\n",
    "    StructField(\"Summary\", StringType(), True),\n",
    "    StructField(\"Text\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Read the data stream from the 'ecommerce-events' Kafka topic\n",
    "kafka_df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", os.getenv(\"KAFKA_BOOTSTRAP_SERVERS\", \"kafka:29092\")) \\\n",
    "    .option(\"subscribe\", \"ecommerce-events\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "# Parse the JSON value from the Kafka message\n",
    "parsed_df = kafka_df.select(from_json(col(\"value\").cast(\"string\"), review_schema).alias(\"data\")).select(\"data.*\")\n",
    "\n",
    "print(\"✅ Kafka stream connected and data parsing is configured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24634f02",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Streaming to Bronze layer started. Path: s3a://bronze/amazon_reviews\n"
     ]
    }
   ],
   "source": [
    "bronze_path = \"s3a://bronze/amazon_reviews\"\n",
    "bronze_checkpoint_path = \"s3a://bronze/checkpoints/amazon_reviews\"\n",
    "\n",
    "bronze_query = parsed_df.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", bronze_checkpoint_path) \\\n",
    "    .start(bronze_path)\n",
    "\n",
    "print(f\"✅ Streaming to Bronze layer started. Path: {bronze_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce5efcd1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Streaming to Silver layer started. Path: s3a://silver/amazon_reviews\n"
     ]
    }
   ],
   "source": [
    "# Perform cleaning and feature engineering\n",
    "silver_df = parsed_df \\\n",
    "    .withColumn(\"review_timestamp\", to_timestamp(col(\"Time\"))) \\\n",
    "    .withColumn(\"helpfulness_ratio\",\n",
    "        when(col(\"HelpfulnessDenominator\") > 0, col(\"HelpfulnessNumerator\") / col(\"HelpfulnessDenominator\"))\n",
    "        .otherwise(0)\n",
    "        .cast(DoubleType())\n",
    "    ) \\\n",
    "    .drop(\"Time\", \"ProfileName\") # Drop redundant or unnecessary columns\n",
    "\n",
    "silver_path = \"s3a://silver/amazon_reviews\"\n",
    "silver_checkpoint_path = \"s3a://silver/checkpoints/amazon_reviews\"\n",
    "\n",
    "silver_query = silver_df.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", silver_checkpoint_path) \\\n",
    "    .start(silver_path)\n",
    "\n",
    "print(f\"✅ Streaming to Silver layer started. Path: {silver_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72fab851",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Streaming to Gold layer started. Path: s3a://gold/amazon_reviews_sentiment\n"
     ]
    }
   ],
   "source": [
    "# Define a User Defined Function (UDF) for sentiment analysis\n",
    "def get_sentiment_score(text):\n",
    "    if text:\n",
    "        return TextBlob(text).sentiment.polarity\n",
    "    return 0.0\n",
    "\n",
    "sentiment_udf = udf(get_sentiment_score, DoubleType())\n",
    "\n",
    "# Add sentiment score to the DataFrame\n",
    "gold_df = silver_df.withColumn(\"sentiment_score\", sentiment_udf(col(\"Text\")))\n",
    "\n",
    "gold_path = \"s3a://gold/amazon_reviews_sentiment\"\n",
    "gold_checkpoint_path = \"s3a://gold/checkpoints/amazon_reviews_sentiment\"\n",
    "\n",
    "gold_query = gold_df.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", gold_checkpoint_path) \\\n",
    "    .start(gold_path)\n",
    "\n",
    "print(f\"✅ Streaming to Gold layer started. Path: {gold_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2ca3c1b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID: 2f09b8ae-c055-48ee-b69a-193b0d1fcd59, Name: None, Status: {'message': 'Processing new data', 'isDataAvailable': True, 'isTriggerActive': True}\n",
      "Query ID: 518865a5-c25d-48ae-93b4-c65128e09d33, Name: None, Status: {'message': 'Processing new data', 'isDataAvailable': True, 'isTriggerActive': True}\n",
      "Query ID: b102dc84-53b8-492f-a41d-245e9d7f602b, Name: None, Status: {'message': 'Initializing sources', 'isDataAvailable': False, 'isTriggerActive': True}\n"
     ]
    }
   ],
   "source": [
    "# Check the status of all active streams\n",
    "for q in spark.streams.active:\n",
    "    print(f\"Query ID: {q.id}, Name: {q.name}, Status: {q.status}\")\n",
    "\n",
    "# To stop a specific query, you can use:\n",
    "# spark.streams.get(query_id).stop()\n",
    "\n",
    "# To stop all queries:\n",
    "# for q in spark.streams.active:\n",
    "#     q.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
