networks:
  lakehouse:
    driver: bridge

volumes:
  minio_data:
  postgres_data:
  spark_data:
  spark_checkpoints: # Added volume for Spark checkpoints

services:
  # =============================================================================
  # 1. LAKEHOUSE CORE - OBJECT STORAGE
  # =============================================================================
  minio:
    image: minio/minio
    container_name: lakehouse-minio
    networks:
      - lakehouse
    ports:
      - "9000:9000" # S3 API Port
      - "9001:9001" # MinIO Console Port
    environment:
      # Credentials must now be provided via environment variables (e.g., in a .env file)
      - MINIO_ROOT_USER=${MINIO_ROOT_USER}
      - MINIO_ROOT_PASSWORD=${MINIO_ROOT_PASSWORD}
    command: server /data --console-address ":9001"
    volumes:
      - minio_data:/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3
    restart: unless-stopped

  # MinIO Client for setup bucket
  minio-setup:
    image: minio/mc:latest
    container_name: lakehouse-minio-setup
    networks:
      - lakehouse
    depends_on:
      minio:
        condition: service_healthy # Wait for MinIO to be healthy
    environment:
      # Credentials must now be provided via environment variables (e.g., in a .env file)
      - MINIO_ROOT_USER=${MINIO_ROOT_USER}
      - MINIO_ROOT_PASSWORD=${MINIO_ROOT_PASSWORD}
    entrypoint: >
      /bin/sh -c "
      mc config host add minio http://minio:9000 $${MINIO_ROOT_USER} $${MINIO_ROOT_PASSWORD};
      mc mb minio/lakehouse --ignore-existing;
      mc mb minio/bronze --ignore-existing;
      mc mb minio/silver --ignore-existing;
      mc mb minio/gold --ignore-existing;
      mc mb minio/models --ignore-existing;
      echo 'MinIO buckets created successfully';
      "

  # =============================================================================
  # 2. STREAMING LAYER - AUTOMQ
  # =============================================================================
  automq:
    image: automqinc/automq:latest
    container_name: lakehouse-automq
    networks:
      - lakehouse
    ports:
      - "9092:9092" # Kafka Protocol Port
      - "9093:9093" # AutoMQ Management Port
      - "8080:8080" # AutoMQ Console Port
    environment:
      # AutoMQ Configuration
      - AUTOMQ_SERVER_ID=1
      - AUTOMQ_LISTENERS=PLAINTEXT://0.0.0.0:9092
      - AUTOMQ_ADVERTISED_LISTENERS=PLAINTEXT://localhost:9092
      - AUTOMQ_CONTROLLER_QUORUM_VOTERS=1@automq:9093
      - AUTOMQ_PROCESS_ROLES=controller,broker
      - AUTOMQ_NODE_ID=1
      - AUTOMQ_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - AUTOMQ_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT
      - AUTOMQ_LISTENERS=PLAINTEXT://0.0.0.0:9092,CONTROLLER://0.0.0.0:9093
      - AUTOMQ_ADVERTISED_LISTENERS=PLAINTEXT://automq:9092
      - AUTOMQ_INTER_BROKER_LISTENER_NAME=PLAINTEXT
      # S3 Storage Configuration (using MinIO)
      - AUTOMQ_S3_ENDPOINT=http://minio:9000
      - AUTOMQ_S3_REGION=us-east-1
      - AUTOMQ_S3_BUCKET=lakehouse
      # Credentials must now be provided via environment variables (e.g., in a .env file)
      - AUTOMQ_S3_ACCESS_KEY=${MINIO_ROOT_USER}
      - AUTOMQ_S3_SECRET_KEY=${MINIO_ROOT_PASSWORD}
      - AUTOMQ_S3_PATH_STYLE=true
      # Log Configuration
      - AUTOMQ_LOG_DIRS=/opt/automq/logs
      - AUTOMQ_NUM_NETWORK_THREADS=8
      - AUTOMQ_NUM_IO_THREADS=8
    volumes:
      - ./automq-logs:/opt/automq/logs
    depends_on:
      minio:
        condition: service_healthy # Wait for MinIO to be healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8080/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

  # =============================================================================
  # 3. ETL & TRANSFORM LAYER - APACHE SPARK
  # =============================================================================
  spark-master:
    image: bitnami/spark:3.5
    container_name: lakehouse-spark-master
    networks:
      - lakehouse
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_MASTER_WEBUI_PORT=8081
      - SPARK_MASTER_PORT=7077
    ports:
      - "8081:8081" # Spark Master Web UI (changed from 8080 to avoid conflict)
      - "7077:7077" # Spark Master Port
    volumes:
      - spark_data:/opt/bitnami/spark/data
      - spark_checkpoints:/opt/bitnami/spark/checkpoints # Mount checkpoint volume
      - ./spark-apps:/opt/bitnami/spark/apps
      - ./datasets:/opt/bitnami/spark/datasets
    restart: unless-stopped

  spark-worker-1:
    image: bitnami/spark:3.5
    container_name: lakehouse-spark-worker-1
    networks:
      - lakehouse
    depends_on:
      spark-master:
        condition: service_healthy # Wait for Spark Master to be healthy
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_WORKER_MEMORY=2g
      - SPARK_WORKER_CORES=2
    volumes:
      - spark_checkpoints:/opt/bitnami/spark/checkpoints # Mount checkpoint volume
      - ./spark-apps:/opt/bitnami/spark/apps
      - ./datasets:/opt/bitnami/spark/datasets
    restart: unless-stopped

  spark-worker-2:
    image: bitnami/spark:3.5
    container_name: lakehouse-spark-worker-2
    networks:
      - lakehouse
    depends_on:
      spark-master:
        condition: service_healthy # Wait for Spark Master to be healthy
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_WORKER_MEMORY=2g
      - SPARK_WORKER_CORES=2
    volumes:
      - spark_checkpoints:/opt/bitnami/spark/checkpoints # Mount checkpoint volume
      - ./spark-apps:/opt/bitnami/spark/apps
      - ./datasets:/opt/bitnami/spark/datasets
    restart: unless-stopped

  # =============================================================================
  # 4. CONTROL CENTER - JUPYTER WITH PYSPARK
  # =============================================================================
  jupyter:
    build:
      context: .
      dockerfile: Dockerfile.jupyter
    container_name: lakehouse-jupyter
    networks:
      - lakehouse
    depends_on:
      spark-master:
        condition: service_healthy # Wait for Spark Master to be healthy
      automq:
        condition: service_healthy # Wait for AutoMQ to be healthy
      minio:
        condition: service_healthy # Wait for MinIO to be healthy
    ports:
      - "8888:8888" # JupyterLab UI
      - "4040:4040" # Spark Application UI
    volumes:
      - ./notebooks:/home/k1/work/notebooks
      - ./datasets:/home/k1/work/datasets
      - ./spark-apps:/home/k1/work/spark-apps
      - ./models:/home/k1/work/models
      - spark_checkpoints:/opt/bitnami/spark/checkpoints # Mount checkpoint volume for access/debugging
    environment:
      - JUPYTER_ENABLE_LAB=yes
      - SPARK_MASTER=spark://spark-master:7077
      # Credentials must now be provided via environment variables (e.g., in a .env file)
      - AWS_ACCESS_KEY_ID=${MINIO_ROOT_USER}
      - AWS_SECRET_ACCESS_KEY=${MINIO_ROOT_PASSWORD}
      - AUTOMQ_BOOTSTRAP_SERVERS=automq:9092
      - MINIO_ENDPOINT=http://minio:9000
    restart: unless-stopped

  # =============================================================================
  # 5. DATA PRODUCER - STREAMING SIMULATION
  # =============================================================================
  data-producer:
    build:
      context: .
      dockerfile: Dockerfile.producer
    container_name: lakehouse-data-producer
    networks:
      - lakehouse
    depends_on:
      automq:
        condition: service_healthy # Wait for AutoMQ to be healthy
    environment:
      - AUTOMQ_BOOTSTRAP_SERVERS=automq:9092
      - AUTOMQ_TOPIC=ecommerce-events
      - PRODUCER_INTERVAL=3 # seconds - faster for demo
    volumes:
      - ./datasets:/app/datasets # Ensure amazon_reviews.csv is here if needed
      - ./producer-scripts:/app/scripts
    restart: unless-stopped

  # =============================================================================
  # 6. AUTOML & MODEL SERVING
  # =============================================================================
  mlflow:
    image: python:3.9-slim
    container_name: lakehouse-mlflow
    networks:
      - lakehouse
    depends_on:
      postgres:
        condition: service_healthy # Wait for Postgres to be healthy
      minio:
        condition: service_healthy # Wait for Minio to be healthy
    ports:
      - "5000:5000"
    environment:
      - MLFLOW_S3_ENDPOINT_URL=http://minio:9000
      # Credentials must now be provided via environment variables (e.g., in a .env file)
      - AWS_ACCESS_KEY_ID=${MINIO_ROOT_USER}
      - AWS_SECRET_ACCESS_KEY=${MINIO_ROOT_PASSWORD}
      - MLFLOW_BACKEND_STORE_URI=postgresql://mlflow:mlflow@postgres:5432/mlflow
      - MLFLOW_DEFAULT_ARTIFACT_ROOT=s3://models/mlflow-artifacts
    command: >
      bash -c "
      pip install mlflow psycopg2-binary boto3 &&
      mlflow server
      --backend-store-uri postgresql://mlflow:mlflow@postgres:5432/mlflow
      --default-artifact-root s3://models/mlflow-artifacts
      --host 0.0.0.0
      --port 5000
      "
    restart: unless-stopped

  # Model Serving API
  model-api:
    build:
      context: .
      dockerfile: Dockerfile.api
    container_name: lakehouse-model-api
    networks:
      - lakehouse
    depends_on:
      mlflow:
        condition: service_healthy # Wait for MLflow to be healthy
      minio:
        condition: service_healthy # Wait for Minio to be healthy
    ports:
      - "8000:8000"
    environment:
      - MINIO_ENDPOINT=http://minio:9000
      # Credentials must now be provided via environment variables (e.g., in a .env file)
      - AWS_ACCESS_KEY_ID=${MINIO_ROOT_USER}
      - AWS_SECRET_ACCESS_KEY=${MINIO_ROOT_PASSWORD}
      - MLFLOW_TRACKING_URI=http://mlflow:5000
    volumes:
      - ./api:/app
      - ./models:/app/models
    restart: unless-stopped

  # =============================================================================
  # 7. DATABASE FOR METADATA
  # =============================================================================
  postgres:
    image: postgres:15
    container_name: lakehouse-postgres
    networks:
      - lakehouse
    environment:
      - POSTGRES_DB=mlflow
      - POSTGRES_USER=mlflow
      - POSTGRES_PASSWORD=mlflow
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U mlflow"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # =============================================================================
  # 8. WEB DASHBOARD
  # =============================================================================
  dashboard:
    build:
      context: .
      dockerfile: Dockerfile.dashboard
    container_name: lakehouse-dashboard
    networks:
      - lakehouse
    depends_on:
      model-api:
        condition: service_healthy # Wait for Model API to be healthy
    ports:
      - "3000:3000"
    environment:
      # These URLs use localhost because the React app runs in the user's browser
      - REACT_APP_API_URL=http://localhost:8000
      - REACT_APP_JUPYTER_URL=http://localhost:8888
      - REACT_APP_SPARK_URL=http://localhost:8081
      - REACT_APP_AUTOMQ_URL=http://localhost:8080
      - REACT_APP_MINIO_URL=http://localhost:9001
      - REACT_APP_MLFLOW_URL=http://localhost:5000
    volumes:
      - ./dashboard:/app
    restart: unless-stopped
