networks:
  lakehouse:
    driver: bridge

volumes:
  minio_data:
  postgres_data:
  spark_data:
  zookeeper_data:
  kafka_data:
  mlflow_data:

services:
  # =============================================================================
  # 1. LAKEHOUSE CORE - OBJECT STORAGE & METADATA
  # =============================================================================
  minio:
    image: minio/minio:latest
    container_name: lakehouse-minio
    networks:
      - lakehouse
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      - MINIO_ROOT_USER=${MINIO_ROOT_USER}
      - MINIO_ROOT_PASSWORD=${MINIO_ROOT_PASSWORD}
    command: server /data --console-address ":9001"
    volumes:
      - minio_data:/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3
    restart: unless-stopped

  minio-setup:
    image: minio/mc:latest
    container_name: lakehouse-minio-setup
    networks:
      - lakehouse
    depends_on:
      minio:
        condition: service_healthy
    environment:
      - AWS_ACCESS_KEY_ID=${MINIO_ROOT_USER}
      - AWS_SECRET_ACCESS_KEY=${MINIO_ROOT_PASSWORD}
      - AWS_ENDPOINT_URL=http://minio:9000
    entrypoint: >
      /bin/sh -c "
      /usr/bin/mc alias set minio http://minio:9000 ${MINIO_ROOT_USER} ${MINIO_ROOT_PASSWORD};
      /usr/bin/mc mb minio/bronze || true;
      /usr/bin/mc mb minio/silver || true;
      /usr/bin/mc mb minio/gold || true;
      /usr/bin/mc mb minio/mlflow || true;
      /usr/bin/mc anonymous set download minio/mlflow;
      exit 0;
      "
    restart: on-failure

  postgres:
    image: postgres:14
    container_name: lakehouse-postgres
    networks:
      - lakehouse
    ports:
      - "5432:5432"
    environment:
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_DB=${POSTGRES_DB}
    volumes:
      - postgres_data:/var/lib/postgresql/data
    restart: unless-stopped

  # =============================================================================
  # 2. STREAMING LAYER - KAFKA
  # =============================================================================
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    container_name: lakehouse-zookeeper
    networks:
      - lakehouse
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    volumes:
      - zookeeper_data:/var/lib/zookeeper/data
    restart: unless-stopped

  kafka:
    image: confluentinc/cp-kafka:7.4.0
    container_name: lakehouse-kafka
    networks:
      - lakehouse
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    volumes:
      - kafka_data:/var/lib/kafka/data
    restart: unless-stopped

  # =============================================================================
  # 3. ETL & TRANSFORM LAYER - APACHE SPARK
  # =============================================================================
  spark-master:
    image: bitnami/spark:3.5
    container_name: lakehouse-spark-master
    networks:
      - lakehouse
    ports:
      - "8080:8080"
      - "7077:7077"
    environment:
      - SPARK_MODE=master
    volumes:
      - spark_data:/opt/bitnami/spark
    restart: unless-stopped

  spark-worker-1:
    image: bitnami/spark:3.5
    container_name: lakehouse-spark-worker-1
    networks:
      - lakehouse
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
    volumes:
      - spark_data:/opt/bitnami/spark
    restart: unless-stopped

  spark-etl-job:
    build:
      context: ./spark-apps
      dockerfile: Dockerfile
    container_name: lakehouse-etl-job
    networks:
      - lakehouse
    depends_on:
      - spark-master
      - kafka
    command:
      - /opt/bitnami/spark/bin/spark-submit
      - --master
      - spark://spark-master:7077
      - --packages
      - "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,io.delta:delta-spark_2.12:3.2.0,org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262"
      - /opt/bitnami/spark/apps/spark_etl_job.py
    volumes:
      - ./spark-apps:/opt/bitnami/spark/apps
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka:29092
      - AWS_ACCESS_KEY_ID=${MINIO_ROOT_USER}
      - AWS_SECRET_ACCESS_KEY=${MINIO_ROOT_PASSWORD}
      - S3_ENDPOINT_URL=http://minio:9000
    # restart: on-failure

  # =============================================================================
  # 4. ML & BATCH TRAINING SERVICES
  # =============================================================================
  mlflow:
    image: python:3.9-slim
    container_name: lakehouse-mlflow
    networks:
      - lakehouse
    depends_on:
      - postgres
      - minio
    ports:
      - "5000:5000"
    environment:
      - AWS_ACCESS_KEY_ID=${MINIO_ROOT_USER}
      - AWS_SECRET_ACCESS_KEY=${MINIO_ROOT_PASSWORD}
      - MLFLOW_S3_ENDPOINT_URL=http://minio:9000
    command: >
      bash -c "pip install mlflow boto3 psycopg2-binary &&
      mlflow server
      --host 0.0.0.0
      --port 5000
      --backend-store-uri postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
      --default-artifact-root s3://mlflow/"
    restart: unless-stopped

  batch-trainer:
    build:
      context: ./batch-trainer
      dockerfile: Dockerfile
    container_name: lakehouse-batch-trainer
    networks:
      - lakehouse
    depends_on:
      - spark-master
      - mlflow
    volumes:
      - ./batch-trainer:/app
    environment:
      - SPARK_MASTER_URL=spark://spark-master:7077
      - MLFLOW_TRACKING_URI=http://mlflow:5000
      - AWS_ACCESS_KEY_ID=${MINIO_ROOT_USER}
      - AWS_SECRET_ACCESS_KEY=${MINIO_ROOT_PASSWORD}
      - MLFLOW_S3_ENDPOINT_URL=http://minio:9000
      - STARTUP_DELAY=120
    command: >
      bash -c "
      echo 'Waiting for ETL job to populate Silver layer...';
      sleep $${STARTUP_DELAY:-120};
      python run_trainer.py
      "
